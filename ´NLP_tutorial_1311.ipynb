{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data_tutorial.csv\") #Import the scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7644887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'[^\\w\\s]' #Regular Expression select everything besides characters and whitespaces.\n",
    "data['body_cleaned'] = data['body'].apply(lambda x: re.sub(pattern, '', x))\n",
    "data['body_cleaned']= data['body_cleaned'].str.replace(\"\\n\",\" \").str.lower() #Our Textcorpora included \"\\n\" indicating linebreaks.\n",
    "# As this does not appear to contain relevant information, we are deleting it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c74874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.cistem import Cistem\n",
    "\n",
    "st = Cistem() #We use this Stemmer, specifically built for the german language.\n",
    "data[\"stemmed\"] = data['body_cleaned'].apply(lambda x: [st.stem(y) for y in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ff452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "data['body_stem_nostop'] = data[\"stemmed\"].apply(lambda x: [item for item in x if item not in german_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size = 0.3, random_state = 123)\n",
    "validation, test = train_test_split(test, test_size = 0.33, random_state = 123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "tf = TfidfVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "\n",
    "cv_data = cv.transform(train['body_stem_nostop'])\n",
    "tf_data = tf.transform(train['body_stem_nostop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cvlog = LogisticRegression().fit(cv_data, train[\"category\"])\n",
    "tflog = LogisticRegression().fit(tf_data, train[\"category\"])\n",
    "\n",
    "test_data_cv = cv.transform([[\"beispiel\", \"wort\", \"fur\", \"vorhersag\"]])\n",
    "test_data_tf = tf.transform([[\"aktiv\", \"konzentration\"]])\n",
    "\n",
    "cvlog.predict(test_data_cv)\n",
    "tflog.predict(test_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "import gensim\n",
    "import urllib.request \n",
    "\n",
    "urllib.request.urlretrieve('https://cloud.devmount.de/d2bc5672c523b086/german.model', 'word2vecgerman.model')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./word2vecgerman.model', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = set(model.index_to_key )\n",
    "train[\"embedd\"] = np.array([np.array([model[i] for i in ls.split(\" \") if i in words]) for ls in train[\"body\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect_avg = []\n",
    "for values in train[\"embedd\"]:\n",
    "    if  values.size:\n",
    "        train_vect_avg.append(np.mean(values, axis=0))\n",
    "    else:\n",
    "        train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "\n",
    "w2vmodel = LogisticRegression().fit(pd.DataFrame(train_vect_avg), train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afaea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('de', if_exists='ignore') \n",
    "ft = fasttext.load_model('cc.de.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"embedd_fasttext\"] = train['body_cleaned'].apply(lambda x: ft.get_sentence_vector(x))\n",
    "fasttext_train = train[\"embedd_fasttext\"].apply(lambda x: pd.Series(x)) #Flatten Arrays into single columns\n",
    "fastmodel = LogisticRegression().fit(fasttext_train, train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(train[\"body_stem_nostop\"],\n",
    "                                   vector_size=500,\n",
    "                                   window=5,\n",
    "                                   min_count=10)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key )\n",
    "\n",
    "train[\"embedd_own\"] = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in train[\"body_stem_nostop\"]])\n",
    "\n",
    "\n",
    "train_vect_avg = []\n",
    "for value in train[\"embedd_own\"]:\n",
    "    if value.size:\n",
    "        train_vect_avg.append(value.mean(axis=0))\n",
    "    else:\n",
    "        train_vect_avg.append(np.zeros(500, dtype=float))\n",
    "\n",
    "selftrainedw2v = LogisticRegression().fit(pd.DataFrame(train_vect_avg), train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7287c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['category_fasttext'] = '__label__' + train['category']\n",
    "fasttexttrain = train['category_fasttext'] + \" \" + train['body_stem_nostop'].apply(lambda word_list: ' '.join(word_list))\n",
    "\n",
    "f_train = open(\"train.txt\", \"a\", encoding=\"utf-8\")\n",
    "for i in range(len(np.array(fasttexttrain))):\n",
    "    f_train.write(np.array(fasttexttrain)[i] + \"\\n\")\n",
    "f_train.close()\n",
    "\n",
    "model_fasttext_selftrained = fasttext.train_supervised(input=\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext_selftrained.predict(\"sauf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "train[\"text\"] = train[\"body\"]\n",
    "train[\"label\"] = train[\"category\"].replace(['ADHS','depression_de'],[0,1])\n",
    "\n",
    "train_transformer = train[[\"label\", \"text\"]]\n",
    "data = Dataset.from_pandas(train_transformer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-german-cased\")\n",
    "\n",
    "def tokenize(dataset):\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length =512)\n",
    "\n",
    "data_tokenized = data.map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    report_to= \"none\",\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    train_dataset=data_tokenized,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d49e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(data_tokenized.select(range(10)))\n",
    "trainer.save_model(\"distillbert_german_classification_reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm import ZeroShotGPTClassifier\n",
    "from skllm.config import SKLLMConfig\n",
    "\n",
    "SKLLMConfig.set_openai_key(\"not used as locally, but needed anyway.\") \n",
    "SKLLMConfig.set_openai_org(\"any string can be used\")\n",
    "\n",
    "clf = ZeroShotGPTClassifier(openai_model=\"gpt4all::mistral-7b-instruct-v0.1.Q4_0.gguf\")\n",
    "\n",
    "clf.fit(None, [\"aufmerksamkeitsdefizit subreddit\", \"depression subreddit\"])\n",
    "clf.predict([\"Example Comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the tfidf Classifier we trained earlier for predictions on the validation set.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tf = TfidfVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "\n",
    "tf_train = tf.transform(train['body_stem_nostop'])\n",
    "tf_val = tf.transform(validation['body_stem_nostop'])\n",
    "\n",
    "tflog = LogisticRegression().fit(tf_train, train[\"category\"])\n",
    "roc_auc_score(validation[\"category\"], tflog.predict_proba(tf_val)[:, 1])\n",
    "#ROCAUC SCore of 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation[\"dataforfasttext\"] = validation[\"body_stem_nostop\"].apply(lambda word_list: ' '.join(word_list))\n",
    "\n",
    "def predict(row):\n",
    "    return model_fasttext_selftrained.predict(row['dataforfasttext'])\n",
    "\n",
    "def process_row(row):\n",
    "    label, value = row\n",
    "    if '__label__ADHS' in label:\n",
    "        return 1 - value[0]\n",
    "    elif '__label__depression_de' in label:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "validation['predictions'] = validation.apply(predict,axis=1)\n",
    "validation[\"proba\"] = validation.predictions.apply(process_row)\n",
    "\n",
    "roc_auc_score(validation[\"category\"], validation['proba']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298388c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "\n",
    "    roc = roc_auc_score(labels, preds[:, 1])\n",
    "    return {\n",
    "          'rocauc': roc,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\49157\\nlptut\\distillbert_german_classification_reddit\", num_labels=2)\n",
    "trainer = Trainer(model=model, args = TrainingArguments(report_to=None, output_dir=\"/\"), compute_metrics=compute_metrics)\n",
    "\n",
    "validation[\"text\"] = validation[\"body\"]\n",
    "validation[\"label\"] = validation[\"category\"].replace(['ADHS','depression_de'],[0,1])\n",
    "\n",
    "validation_transformer = validation[[\"label\", \"text\"]]\n",
    "validation = Dataset.from_pandas(validation_transformer)\n",
    "\n",
    "validation_tokenized = validation.map(tokenize)\n",
    "\n",
    "trainer.evaluate(eval_dataset=validation_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"body\"]\n",
    "test[\"label\"] = test[\"category\"].replace(['ADHS','depression_de'],[0,1])\n",
    "\n",
    "test_transformer = test[[\"label\", \"text\"]]\n",
    "testdata = Dataset.from_pandas(test_transformer)\n",
    "\n",
    "testdata_tokenized = testdata.map(tokenize)\n",
    "\n",
    "trainer.evaluate(eval_dataset=testdata_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
