{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library for working with data tables\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file named \"data_tutorial.csv\"\n",
    "data = pd.read_csv(\"data_tutorial.csv\")\n",
    "\n",
    "# Note: This assumes the dataset is in the same directory as this script. \n",
    "#This should be the case, when cloning the whole github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7644887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the re module for regular expressions\n",
    "import re\n",
    "\n",
    "# Define a regular expression pattern to select everything besides characters and whitespaces\n",
    "pattern = r'[^\\w\\s]'\n",
    "\n",
    "# Apply the pattern to clean up the 'body' column and create a new 'body_cleaned' column\n",
    "data['body_cleaned'] = data['body'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Remove '\\n' characters, convert to lowercase, and update the 'body_cleaned' column\n",
    "data['body_cleaned'] = data['body_cleaned'].str.replace(\"\\n\", \" \").str.lower()\n",
    "\n",
    "# Note: The '\\n' characters were present in the text and are removed as they don't seem to contain relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c74874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Cistem stemmer from the NLTK library, designed for the German language\n",
    "from nltk.stem.cistem import Cistem\n",
    "\n",
    "# Create an instance of the Cistem stemmer\n",
    "stemmer = Cistem()\n",
    "\n",
    "# Apply the stemmer to each word in the 'body_cleaned' column and create a new 'stemmed' column\n",
    "# We split the comments into words using spaces as the default separator.\n",
    "data[\"stemmed\"] = data['body_cleaned'].apply(lambda x: [stemmer.stem(word) for word in x.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ff452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords for the German language from the NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Access the collection of German stopwords from NLTK\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Remove stopwords from the 'stemmed' column and create a new 'body_stem_nostop' column\n",
    "data['body_stem_nostop'] = data[\"stemmed\"].apply(lambda x: [word for word in x if word not in german_stop_words])\n",
    "\n",
    "# Note: Words that are stopwords in the German language are removed from the list of word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=123)\n",
    "\n",
    "# Further split the testing set into validation (20%) and final testing (10%) sets\n",
    "validation, test = train_test_split(test, test_size=0.33, random_state=123)\n",
    "\n",
    "# Note: Out of the remaining 30%, 20% is used as validation data for making model decisions,\n",
    "# such as choosing hyperparameters or feature extraction methods.\n",
    "# The remaining 10% is saved for a final one-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for text vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Train CountVectorizer and TfidfVectorizer on our text data without using any specific settings\n",
    "count_vectorizer = CountVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "\n",
    "# Transform the text data into vectors using the trained vectorizers\n",
    "cv_data = count_vectorizer.transform(train['body_stem_nostop'])\n",
    "tf_data = tfidf_vectorizer.transform(train['body_stem_nostop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression model from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train two logistic regression models on our vectorized data\n",
    "# One using CountVectorizer (cv_data) and the other using TF-IDF (tf_data)\n",
    "cv_logistic_model = LogisticRegression().fit(cv_data, train[\"category\"])\n",
    "tf_logistic_model = LogisticRegression().fit(tf_data, train[\"category\"])\n",
    "\n",
    "# Test the trained models with two random examples.\n",
    "# Note: The input data must match the format used during training with the vectorizer.\n",
    "test_data_cv = count_vectorizer.transform([[\"example\", \"word\", \"for\", \"prediction\"]])\n",
    "test_data_tf = tfidf_vectorizer.transform([[\"active\", \"concentrate\"]])\n",
    "\n",
    "# Predict the category for the test data using each model\n",
    "prediction_cv = cv_logistic_model.predict(test_data_cv)\n",
    "prediction_tf = tf_logistic_model.predict(test_data_tf)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Prediction using CountVectorizer:\", prediction_cv)\n",
    "print(\"Prediction using TF-IDF:\", prediction_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the Gensim library\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# Download the pre-trained German Word2Vec model from a URL and save it locally\n",
    "url = 'https://cloud.devmount.de/d2bc5672c523b086/german.model'\n",
    "local_path = 'word2vecgerman.model'\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Load the German Word2Vec model from the local file\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(local_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "words = set(model.index_to_key )\n",
    "\n",
    "# Extract unique words from the Word2Vec model\n",
    "words = set(model.index_to_key)\n",
    "\n",
    "# Assuming 'train' is your DataFrame with a 'body' column\n",
    "train[\"embedd\"] = train[\"body\"].apply(lambda ls: np.array([model[i] for i in ls.split(\" \") if i in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store average word embeddings for each comment\n",
    "train_vect_avg = []\n",
    "\n",
    "# Calculate the average word embedding for each comment in the 'embedd' column\n",
    "for values in train[\"embedd\"]:\n",
    "    if values.size:\n",
    "        train_vect_avg.append(np.mean(values, axis=0))\n",
    "    else:\n",
    "        # If there are no word embeddings for a comment, append a zero-filled array\n",
    "        train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "\n",
    "# Train a logistic regression model using the average word embeddings and the subreddits.\n",
    "w2v_model = LogisticRegression().fit(pd.DataFrame(train_vect_avg), train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afaea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fasttext module\n",
    "import fasttext.util\n",
    "\n",
    "# Download the pre-trained German language model if it doesn't exist locally\n",
    "fasttext.util.download_model('de', if_exists='ignore')\n",
    "\n",
    "# Load the model\n",
    "ft = fasttext.load_model('cc.de.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'embedd_fasttext' by applying the fastText model to each cleaned comment\n",
    "train[\"embedd_fasttext\"] = train['body_cleaned'].apply(lambda x: ft.get_sentence_vector(x))\n",
    "\n",
    "# Flatten arrays in the 'embedd_fasttext' containing the values per vector into separate columns\n",
    "fasttext_train = train[\"embedd_fasttext\"].apply(lambda x: pd.Series(x))\n",
    "\n",
    "# Train a logistic regression model using the fastText embeddings and corresponding subreddits.\n",
    "fasttext_model = LogisticRegression().fit(fasttext_train, train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Word2Vec model from the Gensim library\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model on the preprocessed wordstems in the 'body_stem_nostop' column\n",
    "w2v_model = Word2Vec(train[\"body_stem_nostop\"],\n",
    "                     vector_size=500,\n",
    "                     window=5,\n",
    "                     min_count=10)\n",
    "\n",
    "# Extract unique words from the Word2Vec model\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "# Create an 'embedd_own' column by applying our selftrained Word2Vec model to each wordstem of each comment\n",
    "train[\"embedd_own\"] = train[\"body_stem_nostop\"].apply(\n",
    "    lambda ls: np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    ")\n",
    "\n",
    "# Calculate the average word embedding for each comment in the 'embedd_own' column\n",
    "train_vect_avg = []\n",
    "for value in train[\"embedd_own\"]:\n",
    "    if value.size:\n",
    "        train_vect_avg.append(value.mean(axis=0))\n",
    "    else:\n",
    "        train_vect_avg.append(np.zeros(500, dtype=float))\n",
    "\n",
    "# Train a logistic regression model using the self-trained Word2Vec embeddings and corresponding subreddit.\n",
    "self_trained_w2v_model = LogisticRegression().fit(pd.DataFrame(train_vect_avg), train[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7287c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'category_fasttext' by adding '__label__' to the existing 'category' column\n",
    "train['category_fasttext'] = '__label__' + train['category']\n",
    "\n",
    "# Combine 'category_fasttext' and preprocessed comments into a single string for each row\n",
    "fasttext_train = train['category_fasttext'] + \" \" + train['body_stem_nostop'].apply(lambda word_list: ' '.join(word_list))\n",
    "\n",
    "# Write the formatted data to a file named \"train.txt\"\n",
    "with open(\"train.txt\", \"a\", encoding=\"utf-8\") as f_train:\n",
    "    for i in range(len(np.array(fasttext_train))):\n",
    "        f_train.write(np.array(fasttext_train)[i] + \"\\n\")\n",
    "\n",
    "# Train a supervised fastText model using the formatted data in \"train.txt\"\n",
    "model_fasttext_selftrained = fasttext.train_supervised(input=\"train.txt\")\n",
    "\n",
    "#Example prediction for a string of wordstems.\n",
    "model_fasttext_selftrained.predict([\"beispiel wort\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch library\n",
    "import torch\n",
    "\n",
    "# Check if a GPU (CUDA) is available for acceleration\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Print the result\n",
    "print(\"CUDA (GPU) is available:\", is_cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the datasets and transformers libraries\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Rename columns and dummycode the outcome.\n",
    "train[\"text\"] = train[\"body\"]\n",
    "train[\"label\"] = train[\"category\"].replace(['ADHS','depression_de'], [0, 1])\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "train_transformer = train[[\"label\", \"text\"]]\n",
    "\n",
    "# Convert the DataFrame to a Dataset, the required format of the transformers library.\n",
    "data = Dataset.from_pandas(train_transformer)\n",
    "\n",
    "# Load the Distilbert Model for tokenization.\n",
    "#Documentation of the model: https://huggingface.co/distilbert-base-german-cased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-german-cased\")\n",
    "\n",
    "# Define a function to tokenize the text in the dataset\n",
    "def tokenize(dataset):\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Tokenize the text in the dataset using the defined function\n",
    "data_tokenized = data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained model for sequence classification with 2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels=2)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    report_to=[],\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_tokenized,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d49e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Predictions and saving the model locally for later use.\n",
    "trainer.predict(data_tokenized.select(range(10)))\n",
    "trainer.save_model(\"distillbert_german_classification_reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from skllm library\n",
    "from skllm import ZeroShotGPTClassifier\n",
    "from skllm.config import SKLLMConfig\n",
    "\n",
    "# Set OpenAI organization information (not used locally, but random string input is required)\n",
    "SKLLMConfig.set_openai_key(\"any string\")\n",
    "SKLLMConfig.set_openai_org(\"any string\")\n",
    "\n",
    "# Initialize a ZeroShotGPTClassifier with a specific openai_model\n",
    "clf = ZeroShotGPTClassifier(openai_model=\"gpt4all::mistral-7b-instruct-v0.1.Q4_0.gguf\")\n",
    "\n",
    "# Fit the classifier with None as input (not used in this case)\n",
    "clf.fit(None, [\"aufmerksamkeitsdefizit subreddit\", \"depression subreddit\"])\n",
    "\n",
    "# Make predictions on a list of example comments\n",
    "predictions = clf.predict([\"Example Comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a TfidfVectorizer on the training data\n",
    "tf = TfidfVectorizer(analyzer=lambda x: x).fit(train['body_stem_nostop'])\n",
    "\n",
    "# Transform the training and validation data into TF-IDF vectors\n",
    "tf_train = tf.transform(train['body_stem_nostop'])\n",
    "tf_val = tf.transform(validation['body_stem_nostop'])\n",
    "\n",
    "# Train a logistic regression model on the TF-IDF vectors\n",
    "tflog = LogisticRegression().fit(tf_train, train[\"category\"])\n",
    "\n",
    "# Calculate the ROC AUC score on the validation set\n",
    "roc_auc = roc_auc_score(validation[\"category\"], tflog.predict_proba(tf_val)[:, 1])\n",
    "\n",
    "# Print the ROC AUC score\n",
    "print(\"ROCAUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the data into the needed format, matching the model input format.\n",
    "validation[\"dataforfasttext\"] = validation[\"body_stem_nostop\"].apply(lambda word_list: ' '.join(word_list))\n",
    "\n",
    "# Define a function to predict using the fastText model on each row\n",
    "def predict(row):\n",
    "    return model_fasttext_selftrained.predict(row['dataforfasttext'])\n",
    "\n",
    "# Define a function that generates the probability per Prediction. \n",
    "#This manual fix is needed as always the probability for the more likely class is given out.\n",
    "\n",
    "def process_row(row):\n",
    "    label, value = row\n",
    "    if '__label__ADHS' in label:\n",
    "        return 1 - value[0]\n",
    "    elif '__label__depression_de' in label:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the predict function to each row and store the predictions in a new column 'predictions'\n",
    "validation['predictions'] = validation.apply(predict, axis=1)\n",
    "\n",
    "# Apply the process_row function to calculate probabilities and store in a new column 'proba'\n",
    "validation[\"proba\"] = validation.predictions.apply(process_row)\n",
    "\n",
    "# Calculate the ROC AUC score on the validation set\n",
    "roc_auc = roc_auc_score(validation[\"category\"], validation['proba'])\n",
    "\n",
    "# Print the ROC AUC score\n",
    "print(\"ROCAUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for sequence classification with 2 labels\n",
    "model_path = \"/distillbert_german_classification_reddit\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define a function to compute metrics, including ROC AUC score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    roc = roc_auc_score(labels, preds[:, 1])\n",
    "    return {'rocauc': roc}\n",
    "\n",
    "# Set up training arguments for the Trainer (not used for evaluation)\n",
    "args = TrainingArguments(report_to=[], output_dir=\"/\")\n",
    "\n",
    "# Initialize the Trainer with the model and training arguments\n",
    "trainer = Trainer(model=model, args=args, compute_metrics=compute_metrics)\n",
    "\n",
    "#Preparing Validationdata\n",
    "validation[\"text\"] = validation[\"body\"]\n",
    "validation[\"label\"] = validation[\"category\"].replace(['ADHS', 'depression_de'], [0, 1])\n",
    "\n",
    "validation_transformer = validation[[\"label\", \"text\"]]\n",
    "\n",
    "validation_data = Dataset.from_pandas(validation_transformer)\n",
    "\n",
    "# Tokenize the text in the validation dataset using the defined function\n",
    "validation_tokenized = validation_data.map(tokenize)\n",
    "\n",
    "# Evaluate the model on the validation dataset using the Trainer\n",
    "evaluation_result = trainer.evaluate(eval_dataset=validation_tokenized)\n",
    "\n",
    "# Print the ROC AUC score\n",
    "print(\"ROCAUC Score:\", evaluation_result['rocauc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing Testdata\n",
    "test[\"text\"] = test[\"body\"]\n",
    "test[\"label\"] = test[\"category\"].replace(['ADHS', 'depression_de'], [0, 1])\n",
    "\n",
    "test_transformer = test[[\"label\", \"text\"]]\n",
    "\n",
    "test_data = Dataset.from_pandas(test_transformer)\n",
    "\n",
    "# Tokenize the text in the test dataset using the defined function\n",
    "test_data_tokenized = test_data.map(tokenize)\n",
    "\n",
    "# Evaluate the model on the test dataset using the Trainer\n",
    "evaluation_result_test = trainer.evaluate(eval_dataset=test_data_tokenized)\n",
    "\n",
    "# Print the ROC AUC score on the test dataset\n",
    "print(\"ROCAUC Score on Test Data:\", evaluation_result_test['rocauc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
